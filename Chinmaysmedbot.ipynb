{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmbhatt1/abstractive-question-answering-system/blob/main/Chinmaysmedbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXASu2vzVWdU"
      },
      "outputs": [],
      "source": [
        "# Installing all the libraries\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install faker\n",
        "!pip install datasets\n",
        "!pip install wikipedia-api\n",
        "%pip install -Uq chromadb numpy datasets\n",
        "!pip install --upgrade gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7iVDZE_el17"
      },
      "outputs": [],
      "source": [
        "#Fetching the API ket from hugging face. This step is not mandatory but if it is not done, we will get a warning.\n",
        "from huggingface_hub import HfApi\n",
        "my_api = HfApi()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8jnhoAxhLZY"
      },
      "outputs": [],
      "source": [
        "#We will be using Bart model for conditional generation from Hugging face.\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained('vblagoje/bart_lfqa')\n",
        "generator = BartForConditionalGeneration.from_pretrained('vblagoje/bart_lfqa')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdrdzDE7vr7Z"
      },
      "outputs": [],
      "source": [
        "#Wikipedia has multiple \"main\" categories and one of them is related to health and fitness (https://en.wikipedia.org/wiki/Wikipedia:Contents/Categories#Health_and_fitness). This main category has numerous other categories attached to it. These are stored\n",
        "#in the next code block named \"list of categories\". Each of these categories has pages associated to them.(not subcategory). These pages need to be scraped.\n",
        "\n",
        "# Instantiate the wikipedia object\n",
        "import wikipediaapi\n",
        "wiki_wiki = wikipediaapi.Wikipedia('healthbot (cmbhatt99@gmail.com)','en')\n",
        "\n",
        "def scrape_category_pages(category_name):\n",
        "  \"\"\"\n",
        "  This function takes in each element of list_of_categories and visits all the pages in it to scrape their summary. Here, \"member\" means the names of the pages/article.\n",
        "  \"\"\"\n",
        "   pages_with_summary = {}\n",
        "\n",
        "    # Get the category page\n",
        "   category_page = wiki_wiki.page(\"Category:\" + category_name)\n",
        "\n",
        "    # Iterate over the category members\n",
        "   for member in category_page.categorymembers.values():\n",
        "        # Check if the member is a page\n",
        "       if member.ns == wikipediaapi.Namespace.MAIN:\n",
        "            # Get the page object\n",
        "           page = wiki_wiki.page(member.title)\n",
        "            # Retrieve the summary of the page\n",
        "           summary = page.summary\n",
        "            # Store the title and summary in the dictionary\n",
        "           pages_with_summary[member.title] = summary\n",
        "\n",
        "   return pages_with_summary\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdqY7KjbH2-F"
      },
      "outputs": [],
      "source": [
        "#looping over a few categories to get the summary of the pages within them. All these categories have multiple articles/pages dedicated to them.\n",
        "list_of_categories = ['Diseases and disorders', 'Pharmacy', 'Symptoms and signs', 'Pathology', 'Mental health', 'Pediatrics','Nutrition','Gynaecology','Ophthalmology','Physical exercise','Health care occupations','Phytochemicals','Symptoms and signs of mental disorders',\n",
        "                      'Neurology','Dietary supplements','Surgery','Occupational safety and health','Chemical substances for emergency medicine','Medical and health organisations based in India',\n",
        "                      'Pharmaceutical companies of India','Cause (medicine)','Symptoms','Symptoms and signs: Urinary system','Symptoms and signs: Circulatory system','Medical associations based in India','Geriatrics','Gastroenterology']\n",
        "summary=[]\n",
        "for category in list_of_categories:\n",
        "  summary.append(scrape_category_pages(category))\n",
        "\n",
        "# Data cleaning. Replace \"\\n\" with \"\".\n",
        "for i in range(len(summary)):\n",
        "  for key, value in summary[i].items():\n",
        "    summary[i][key] = value.replace(\"\\n\",\"\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A category list of diseases is structured a bit differently. It has pages named \"list of diseases(A), list of_diseases(b)....\".\n",
        "#These pages have further links to the actual disease articles and pages so the previous scraping technique doesn't work.\n",
        "dict_of_diseases = {}\n",
        "category_page_diseases = wiki_wiki.page('Category:' + 'Lists of diseases')\n",
        "for member in category_page_diseases.categorymembers.values():\n",
        "  if member.ns == wikipediaapi.Namespace.MAIN:\n",
        "    page_py = wiki_wiki.page(member.title)\n",
        "    for key, value in page_py.links.items():\n",
        "      dict_of_diseases[key] = wiki_wiki.page(key).summary\n",
        "\n",
        "knowledge_base_diseases = []\n",
        "for key, value in dict_of_diseases.items():\n",
        "  if len(value)>5:\n",
        "    knowledge_base_diseases.append(value.replace(\"\\n\",\"\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XBbmrpoKeCcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SwSM_TTcxrw"
      },
      "outputs": [],
      "source": [
        "# We will be converting our knowlege base/context into vectors and storing it in chroma db which is open source.\n",
        "#We have removed the key and only taken values from our knowledge base. Knowledge base was in this format [{\"headache\":\"Headache, also known as cephalalgia, is the symptom...}\"]\n",
        "#but there is no point storing the keys. Additionally, we can only store these values in a list and not a dictionary.\n",
        "import chromadb\n",
        "\n",
        "client = chromadb.Client()\n",
        "\n",
        "collection = client.create_collection(\"Chinmaymedbot1\")\n",
        "\n",
        "knowledge_base = [value for d in summary for value in d.values()]\n",
        "\n",
        "knowledge_base.extend(knowledge_base_diseases)\n",
        "\n",
        "collection.add(\n",
        "    ids=[str(i) for i in range(len(knowledge_base))],  # IDs are just strings\n",
        "    documents=knowledge_base,\n",
        "    metadatas=[{\"type\": \"support\"} for _ in range(len(knowledge_base))\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Eiw84pNylLM"
      },
      "outputs": [],
      "source": [
        "# This a synthetic data generated with the help of faker. It contains random patient names, doctors and some data you might see in a hospital database.\n",
        "import pandas as pd\n",
        "import random\n",
        "from faker import Faker\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "random.seed(42)\n",
        "# Initialize Faker library\n",
        "fake = Faker()\n",
        "\n",
        "# Initialize lists to store data\n",
        "data = {\n",
        "    'id': [],\n",
        "    'Name': [],\n",
        "    'Visit_number': [],\n",
        "    'doctor_name': [],\n",
        "    'specialty':[],\n",
        "    'Expected time in minutes':[],\n",
        "    'last_visit': []\n",
        "}\n",
        "\n",
        "# List of doctor specialties\n",
        "specialties = ['Dentist', 'ENT', 'Opthalmalogist', 'Gynaecologist', 'Orthopedist', 'Pediatrician']\n",
        "\n",
        "\n",
        "# Generate synthetic data\n",
        "for i in range(1000):\n",
        "    data['id'].append(i + 1)\n",
        "    data['Name'].append(fake.name())\n",
        "    data['Visit_number'].append(random.randint(1, 5))  # Random visit number between 1 and 5\n",
        "    data['doctor_name'].append(fake.name())\n",
        "    specialty = random.choice(specialties)\n",
        "    data['specialty'].append(specialty)\n",
        "    data['Expected time in minutes'].append(random.randint(10,30)),\n",
        "\n",
        "    # Generate last visit date (within the last year)\n",
        "    last_visit_date = fake.date_time_between(start_date='-1y', end_date='now')\n",
        "    data['last_visit'].append(last_visit_date.strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A5miODVVMzn"
      },
      "outputs": [],
      "source": [
        "# Created a chat interface which can be used by both patients and hospital management. The patients have to type in yes and the medbot will try to answer questions related to healthcare domain. If no is typed, the bot will ask for a patient id which when provided fetches detail of the patient.\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "def respond(message):\n",
        "        results = collection.query(\n",
        "                  query_texts=message,\n",
        "                  n_results=5)\n",
        "\n",
        "        conditioned_doc = \"<P> \" + \" <P> \".join([d for d in results['documents'][0]])\n",
        "        query_and_docs = \"question: {} context: {}\".format(message, conditioned_doc)\n",
        "\n",
        "        model_input = tokenizer(query_and_docs, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "        generated_answers_encoded = generator.generate(input_ids=model_input[\"input_ids\"],\n",
        "                                           attention_mask=model_input[\"attention_mask\"],\n",
        "                                           min_length=10,\n",
        "                                           max_length=256,\n",
        "                                           do_sample=False,\n",
        "                                           early_stopping=True,\n",
        "                                           num_beams=8,\n",
        "                                           temperature=1.0,\n",
        "                                           top_k=None,\n",
        "                                           top_p=None,\n",
        "                                           eos_token_id=tokenizer.eos_token_id,\n",
        "                                           no_repeat_ngram_size=3,\n",
        "                                           num_return_sequences=1)\n",
        "\n",
        "        answer_to_the_query = tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "        time.sleep(2)\n",
        "        return answer_to_the_query[0]\n",
        "\n",
        "def management_query(patient_id):\n",
        "  return f'Patient: {df.loc[df[\"id\"] == patient_id, \"Name\"].values[0]},\\nVisit number: {df.loc[df[\"id\"] == patient_id, \"Visit_number\"].values[0]},\\nDoctor name: Dr. {df.loc[df[\"id\"] == patient_id, \"doctor_name\"].values[0]},\\nSpecialty: {df.loc[df[\"id\"] == patient_id, \"specialty\"].values[0]},\\nExpected time: {df.loc[df[\"id\"] == patient_id, \"Expected time in minutes\"].values[0]},\\nLast visit: {df.loc[df[\"id\"] == patient_id, \"last_visit\"].values[0]}'\n",
        "\n",
        "\n",
        "def chat_func(message, history):\n",
        "  if len(history)==0:\n",
        "    if message.lower()==\"yes\":\n",
        "        return \"Ask me anything!\"\n",
        "    elif message.lower()==\"no\":\n",
        "        return \"Please put in the patient id\"\n",
        "  else:\n",
        "    if history[0][0]==\"no\":\n",
        "      id_message = int(message)\n",
        "      return management_query(id_message)\n",
        "    else:\n",
        "      history.append((message, respond(message)))\n",
        "      return respond(message)\n",
        "\n",
        "\n",
        "gr.ChatInterface(\n",
        "    chat_func,\n",
        "    chatbot=gr.Chatbot(height=300),\n",
        "    textbox=gr.Textbox(placeholder=\"Type yes if you are a patient\", container=False, scale=7),\n",
        "\n",
        "    title=\"Chinmay's Medbot\",\n",
        "    description=\"Abstractive question answering system\",\n",
        "    theme=\"soft\",\n",
        ").launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMmcS2ghyyyYY8DskfNsYDD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
